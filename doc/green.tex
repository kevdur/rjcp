% !TEX engine = xelatex
%
% Notes on reversible jump Markov chain Monte Carlo
%

\documentclass[11pt,a4paper]{article}
  
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else
  \usepackage{unicode-math}
\fi
\usepackage{hyperref}

% Custom maths commands %=======================================================

\newcommand\ds{\displaystyle}                    % large maths.
\newcommand\ts{\textstyle}                       % small maths.
\newcommand\mb[1]{\mathbb{#1}}                   % mathbb shorthand.
\newcommand\mc[1]{\mathcal{#1}}                  % mathcal shorthand.
\newcommand\ff[1]{^{\underline{#1}}}             % falling factorial.
\newcommand\rf[1]{^{\overline{#1}}}              % rising factorial.
\newcommand\ul[1]{\underline{#1}}                % underline.
\newcommand\ol[1]{\overline{#1}}                 % overline.
\DeclareMathOperator\Pb{P}                       % probability.
\DeclareMathOperator\Ex{E}                       % expected value.
\DeclareMathOperator\Va{V}                       % variance.
\DeclarePairedDelimiter\lr{\lparen}{\rparen}     % sized parentheses.
\DeclarePairedDelimiter\lrb{\lbrack}{\rbrack}    % sized brackets.
\DeclarePairedDelimiter\abs{\lvert}{\rvert}      % absolute value symbol.
\DeclarePairedDelimiter\cl{\lceil}{\rceil}       % ceiling symbol.
\DeclarePairedDelimiter\fl{\lfloor}{\rfloor}     % floor symbol.

\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{defn}{Definition}

% Title %=======================================================================

\title{Notes on reversible jump Markov chain Monte Carlo}
\author{}
\date{February 2018}

% Document %====================================================================

\begin{document}

\maketitle

\section[]{Reversible jump Markov chain Monte Carlo\footnote{See Peter Green,
\textit{Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination}, 1995. Much of the current exposition is also based on Rasmus
Waagepetersen, \textit{A tutorial on reversible jump MCMC with a view towards
applications in QTL mapping}, 2001.}} %=========================================
\label{sec:rjmc}

\subsection{Introduction} %=====================================================
\label{sec:intro}

Consider the scenario in which we have a data set that we assume has been
generated by a particular model. Let~$\mc{X}$ be the state space of this model,
so that each state~$x \in \mc{X}$ is a realised parameter vector---often, but
not always, of real numbers.

Let~$\pi(dx)$ denote the target distribution that we are interested in---usually
the posterior distribution for the model parameters given the data. We
assume~$\pi$ is a probability measure\footnote{See Section~\ref{sec:measure} for
some introductory measure theory.} on~$\mc{X}$, known up to a normalising
constant. In standard Markov chain Monte Carlo, we define a conditional
probability measure~$P(x,dx')$ (a probability kernel) that gives transition
probabilities between states in~$\mc{X}$; if~$P$ is irreducible (so all states
can be reached from any initial state), aperiodic (there are no states that can
only be returned to in multiples of a certain number of steps), and reversible,
then samples drawn from a run of the Markov chain defined by~$P$ approximate the
target distribution~$\pi$. More specifically, $\pi$ is the stationary
distribution of the Markov chain.

Practically, the most important of the above properties is reversibility. The
Markov chain defined by~$P$ is said to be reversible with respect to~$\pi$ if it
satisfies a property called detailed balance, which requires that for all
measurable $A,B \subset \mc{X}$,
\begin{equation}\label{eqn:balance mh}
  \int_A \int_B \pi(dx)P(x,dx') = \int_B \int_A \pi(dx')P(x',dx)
\end{equation}
In Metropolis-Hastings MCMC, the transition measure~$P(x,dx')$ is made up of a
proposal distribution~$q(x,A)$ and an acceptance density~$\alpha(x,x')$, such
that $P(x,dx') = q(x,dx')\alpha(x,x')$, and in this case, there is a choice
of~$\alpha$ that always leads to detailed balance:
\begin{equation}\label{eqn:alpha mh}
  \alpha(x,x') = \min\lr*{1,\frac{\pi(dx')q(x',dx)}{\pi(dx)q(x,dx')}}.
\end{equation}

To be clear, we have assumed that~$\pi(dx)$ is only known as a density function
up to a constant factor, say~$\pi(dx) = p(x)d\pi/Z$. Fortunately, this is no
real restriction, since $\pi(dx')/\pi(dx)$ can still be computed as
\[ \frac{\pi(dx')}{\pi(dx)} = \frac{p(x')/Z}{p(x)/Z} = \frac{p(x')}{p(x)}. \]
Usually, $\pi(dx)$ is the posterior distribution of a model~$M$'s parameters~$x
\equiv \theta$, given data~$y$:
\[ \pi(A) = \int_A p(\theta \mid y,M)\,d\theta
  = \int_A p(y \mid \theta,M)p(\theta \mid M)/p(y \mid M)\,d\theta, \]
in which case~$Z = p(y \mid M)$.

\subsection{Multiple parameter spaces} %========================================
\label{sec:multiple}

In the more general case addressed by reversible jump MCMC, the state
space~$\mc{X}$ may consist of disjoint, disparate subspaces corresponding to a
countable set of different models, and the Markov chain may move between them in
much the same way as it does within them.

In single-model Metropolis-Hastings simulations, it is quite common to break the
transition process into simpler parts by applying a number of different moves,
either in sequence or according to a predefined probability vector. One of the
notable contributions of reversible jump MCMC is the possibility for
state-dependent mixing, in which the set of proposed moves may depend on the
current state of the Markov chain.

For example, we might have a countable set of moves indexed by a set~$M$, and a
conditional probability~$q_m(x,dx')$ of proposing a move of type~$m$ to
state~$x'$, given that the current state of the chain is~$x$. These
probabilities need not sum to 1, so that with probability~$1 - \sum_m
q_m(x,\mc{X})$ no move is proposed. Also, it is reasonable to restrict the moves
that are available from any given state, so it is quite possible
that~$q_m(x,\mc{X}) = 0$ for arbitrarily many~$m$. As in the previous section,
each move is paired with an acceptance probability~$\alpha_m(x,x')$, which will
be derived abstractly below, and in full for the real case later on. The
probability of remaining at the current state~$x$ is then:
\[ s(x) = 1 - \sum_{m\in M} q_m(x,\mc{X})
  + \sum_m \int_{\mc{X}} q_m(x,dx')\lr*{1 - \alpha_m(x,x')}, \]
and the transition measure~$P$ takes the form:
\[ P(x,B) = \sum_m \int_B q_m(x,dx')\alpha_m(x,x') + s(x)[x \in B]. \]
Here, $B$ is a measurable subset of~$\mc{X}$, and~$[x \in B]$ is the Iverson
bracket, or indicator function.

In terms of reversibility of the chain, state-dependent mixing imposes no real
restrictions, because for detailed balance to hold overall, it need only hold
for each move individually. Substituting the above form for~$P(x,B)$
into~\eqref{eqn:balance mh} gives:
\begin{multline*}
  \sum_m \int_A \pi(dx) \int_B q_m(x,dx')\alpha_m(x,x')
    + \int_{A\cap B} \pi(dx)s(x) \\
  = \sum_m \int_B \pi(dx') \int_A q_m(x',dx)\alpha_m(x',x)
    + \int_{B\cap A} \pi(dx')s(x'),
\end{multline*}
which is satisfied if for each~$m$, $A$, and~$B$,
\begin{equation}\label{eqn:balance rj}
\begin{multlined}
  \int_A \pi(dx) \int_B q_m(x,dx')\alpha_m(x,x') \\
  = \int_B \pi(dx') \int_A q_m(x',dx)\alpha_m(x',x).
\end{multlined}
\end{equation}

There is an important difference between this equation and~\eqref{eqn:balance
mh}: in contrast with the previous section, the measures being integrated with
respect to here are not necessarily interchangeable. If~$q_m$ is a
model-switching move, and~$A$ and~$B$ belong to parameter spaces~$S_1$ and~$S_2$
respectively, then in the first integral we are concerned with a measure
on~$S_1\times S_2$ (it may be null elsewhere on~$\mc{X}\times\mc{X}$), whereas
the measure in the second integral has to do with~$S_2\times S_1$.
For~\eqref{eqn:balance rj} to hold, we must first know that both integrals can
at least be expressed with respect to the same measure; we thus make the
following assumption: let~$\mu_m$ be a measure on~$\mc{X}\times\mc{X}$ relative
to which~$\pi q_m$ has a finite, computable density on~$(S_1\times S_2) \cup
(S_2\times S_1)$. Also assume that~$\mu_m$ is symmetric, in the sense that it
matters not whether it is applied on~$S_1\times S_2$ or~$S_2\times S_1$, i.e.,
$\mu_m(dx,dx') = \mu_m(dx',dx)$. (This is all not quite as abstract and
circuitous as it may sound---as Section~\ref{sec:reals} will hopefully show.)

With respect to~$\mu_m$, assume that~$\pi(dx)q_m(x,dx')$ has density~$f_1(x,x')$
and~$\pi(dx')q_m(x',dx)$ density~$f_2(x',x)$. Then detailed balance holds (for
move~$m$) if we let
\begin{equation}\label{eqn:alpha rj}
\begin{gathered}
  \alpha_m(x,x') = \min\lr*{1,\frac{f_2(x',x)}{f_1(x,x')}} \\
  \text{and} \\
  \alpha_m(x',x) = \min\lr*{1,\frac{f_1(x',x)}{f_2(x,x')}},
\end{gathered}
\end{equation}
(we have simply made explicit the fact that~$\alpha$ is defined separately
on~$S_1\times S_2$ and~$S_2\times S_1$), because this implies that
\[ f_1(x,x')\alpha_m(x,x') = f_2(x',x)\alpha_m(x',x), \]
which in turn is all that was required to prove~\eqref{eqn:balance rj}:
\begin{align*}
  \int_A \int_B \pi(dx)q_m(x,dx')\alpha_m(x,x')
  &= \int_A \int_B \mu_m(dx,dx')f_1(x,x')\alpha_m(x,x') \\
  &= \int_B \int_A \mu_m(dx',dx)f_2(x',x)\alpha_m(x',x) \\
  &= \int_B \int_A \pi(dx')q_m(x',dx)\alpha_m(x',x).
\end{align*}

\subsection{Real parameter spaces} %============================================
\label{sec:reals}

Let~$\mc{X}$ be a state space consisting of real-valued vectors, so $\mc{X} =
\cup_{i\in I} S_i$, with each~$S_i \subseteq \mb{R}^{n_i}$, for some countable
index set~$I$. We will assume the Borel $\sigma$-algebra on~$\mc{X}$, but won't
refer to it directly. Let~$\pi$ again be the target distribution we wish to
sample from (a probability measure on~$\mc{X}$), and assume that for each
subspace~$S_i$ the measure~$\pi$ has a non-negative, finite density~$p_i/Z_i$
with respect to Lebesgue measure~$\lambda_{n_i}$ on~$S_i$.

Following on from the previous section, we can restrict ourselves to moves
between a pair of subspaces, say~$S_1$ and~$S_2$. Let~$q_1(x,dx')$
and~$q_2(x',dx)$ be the proposal measures (technically, these are kernels)
on~$S_1 \times S_2$ and~$S_2 \times S_1$ (i.e., for moves~$S_1 \to S_2$ and~$S_2
\to S_1$) respectively. Also assume that these moves are only attempted with
probabilities~$j_1(x)$ and~$j_2(x')$, respectively.

The most obvious practical scenario (and the only one we address) is one in
which moves can be made with the aid of auxiliary variables, and we have an idea
of how these random variables should be generated. In the simplest case, we
might have~$n_1 < n_2$, and need $n_2-n_1$ random numbers~$u$ to map a state~$x
\in S_1$ to a proposed state~$x' \in S_2$. More generally, $u$ might
contain~$m_1$ auxiliary numbers, so that~$(x,u) \in \mb{R}^{n_1} \times
\mb{R}^{m_1} = \mb{R}^n$ (say).

We assume that the proposed state~$x'$ is obtained with the aid of some
bijection~$h\colon \mb{R}^n \to \mb{R}^n$ that we have access to, so that
$h(x,u) = (x',u')$, where $u'$ is a vector of~$m_2 = n-n_2$ auxiliary variables.
The so-called `dimension-matching' assumption of reversible jump MCMC is that we
use~$h^{-1}$ when we perform moves in the reverse direction; it implies that a
vector of auxiliary variables~$u'$ will need to be generated when proposing a
state in~$S_1$ given~$x' \in S_2$ as well. We therefore need to assume that we
have access to densities~$g_1(u)$ and~$g_2(u')$, with respect to~$\lambda_{m_1}$
and~$\lambda_{m_2}$, that can be used to augment states in~$S_1$ and~$S_2$
respectively.

It is important (for reasons we'll point out below) that the bijection~$h$ is a
diffeomorphism---that is, both~$h$ and~$h^{-1}$ are differentiable---and also
that the intermediate dimension~$n$ is minimal, so that there are no superfluous
auxiliary variables. To be specific, $(x,x')$ must determine
$(u,u')$---otherwise the move~$x \to x'$ could not be expressed using~$g_1(u)$
for a particular~$u$. We already know that $(x,u)$ determines~$x'$ via~$h$ and a
projection onto~$S_2$; if we set~$r_x(u) = x'$, the fact that $(x,x')$
determines $(u,u')$ will imply that~$r_x^{-1}$ exists.

Let~$A \subseteq S_1$ and~$B \subseteq S_2$. We have two measures, on~$S_1
\times S_2$ and~$S_2 \times S_1$ respectively:
\begin{equation}\label{eqn:measures}
\begin{gathered}
  \int_A \lambda_{n_1}(dx)\,p_1(x)j_1(x) q_1(x,B) \\
    \text{and} \\ \int_B \lambda_{n_2}(dx')\,p_2(x')j_2(x') q_2(x',A).
\end{gathered}
\end{equation}
Consider the first measure, which gives the probability of proposing a move from
a state in~$A$ to one in~$B$. We wish to define the proposal measure~$q_1$ in
such a way that its density (with respect to some new measure on~$S_2$) can be
expressed in terms of~$g_1$, which is a density on~$\mb{R}^{m_1}$. In
particular, we require a measure~$\mu_1$ on~$S_2$ for which
\begin{equation}\label{eqn:mu_1}
  \int_B (g_1 \circ r_x^{-1})\,d\mu_1 = \int_{r_x^{-1}(B)} g_1\,d\lambda_{m_1},
\end{equation}
since this implies that the probability of proposing some~$x' \in S_2$ is
simply~$g_1(u')$, where~$u = r_x^{-1}(x')$. The change of variables theorem for
integrals has just this form: set
\[ \mu_1(B) = \lambda_{m_1}(r_x^{-1}(B))
  = \lambda_{m_1}(\{\, u : r_x(u) \in B \,\}); \]
then $\mu_1$ is a positive measure on~$S_2$, and for any measurable~$f\colon S_2
\to \mb{R}$:
\[ \int_{S_2} f\,d\mu_1 = \int_{\mb{R}^{m_1}} (f \circ r_x)\,d\lambda_{m_1}. \]
Equation~\ref{eqn:mu_1} follows when~$f = g_1 \circ r_x^{-1}$ on~$B$:
\begin{align*}
  \int_B (g_1 \circ r_x^{-1})\,d\mu_1
    &= \int_{S_2} \symbf{1}_B \cdot (g_1 \circ r_x^{-1})\,d\mu_1 \\
    &= \int_{\mb{R}^{m_1}} (\symbf{1}_B \circ r_x) \cdot g_1\,d\lambda_{m_1}
    = \int_{r_x^{-1}(B)} g_1\,d\lambda_{m_1}.
\end{align*}

The first measure in~\eqref{eqn:measures} can now be rewritten as
\begin{equation}\label{eqn:measure 1}
  \int_A \lambda_{n_1}(dx)\,p_1(x)j_1(x)
      \int_B \mu_1(dx')\,g_1(r_x^{-1}(x')),
\end{equation}
and since the product of two measures is a measure on the relevant product
space, we have
\[ \int_A \int_B \mu(dx,dx')\,p_1(x)j_1(x)g_1(r_x^{-1}(x')), \]
in which the product measure is
\[ \mu(A \times B) = \lambda_{n_1}(A)\mu_1(B)
  = \lambda_n(\{\, (x,u) : x \in A, r_x(u) \in B \,\}). \]
This measure is in fact sufficient for our purposes, for two reasons: one,
because it can also be applied to~$S_2 \times S_1$, making it symmetric, and
two, because both measures in~\eqref{eqn:measures} have computable densities
with respect to~$\mu$.

The density for moves from~$S_1$ to~$S_2$ is clear:
\[ f_1(x,x') = p_1(x)j_1(x)g_1(r_x^{-1}(x')). \]
The second density---on~$S_2 \times S_1$---must exist by the Lebesgue
decomposition and Radon-Nikodym theorems, and can also be given explicitly,
because in this case the Radon-Nikodym derivative is the determinant of the
Jacobian of~$h^{-1}\colon \mb{R}^n \to \mb{R}^n$.

To see this, note firstly that if we apply all of the above steps to the second
expression in~\eqref{eqn:measures}, then the resulting measure on~$S_2 \times
S_1$ is
\begin{equation}\label{eqn:measure 2}
  \int_B \lambda_{n_2}(dx')\,p_2(x')j_2(x')
    \int_A \mu_2(dx)\,g_2(r_{x'}^{-1}(x)),
\end{equation}
mirroring~\eqref{eqn:measure 1}. The new functions are straightforward: we have
$r_{x'}(u') = x'$, and~$\mu_2(A) = \lambda_{m_2}(r_{x'}^{-1}(A))$. Together,
$\lambda_{n_2}$ and~$\mu_2$ provide a measure on~$S_2 \times S_1$ (as above):
\[ \mu^*(B \times A) = \lambda_{n_2}(B)\mu_2(A)
  = \lambda_n(\{\, (x',u') : x' \in B, r_{x'}(u') \in A \,\}). \]
If~$\mu(B \times A) = \mu(A \times B)$ is the Lebesgue measure of the region~$R
\subseteq \mb{R}^n$, then~$\mu^*(B \times A)$ is the Lebesgue measure of~$h(R)$.
In addition, $\mu^*$ is absolutely continuous relative to~$\mu$, because if~$R$
is a null set of~$\mu$, one of its elements is determined by the other~$n-1$,
and since~$h$ is one-to-one, the same must be true of~$h(R)$. Hence~$\mu^*$ has
a Radon-Nikodym derivative with respect to~$\mu$, and~\eqref{eqn:measure 2}
implies
\begin{multline*}
  \int_B \int_A \mu^*(dx',dx)\,p_2(x')j_2(x')g_2(r_{x'}^{-1}(x)) \\
    = \int_B \int_A \mu(dx,dx')\,\frac{d\mu^*}{d\mu}
        p_2(x')j_2(x')g_2(r_{x'}^{-1}(x)).
\end{multline*}
Since $d\mu^*/d\mu$ is the relative rate of change of~$\mu^*$ with respect
to~$\mu$, it is the factor by which~$h$ scales volume, and is thus given by the
absolute value of the determinant of the Jacobian matrix of~$h$. The Jacobian
exists for any differentiable mapping, but its determinant is only non-zero if
the function's inverse is also differentiable---hence the diffeomorphism
requirement above. In particular then, the density of the joint target-proposal
measure on~$S_2 \times S_1$ relative to~$\mu$ is
\[ f_2(x',x) = \abs*{\frac{\partial(x',u')}{\partial(x,u)}}\,
    p_2(x')j_2(x')g_2(r_{x'}^{-1}(x)), \]
and we are now in a position to prove that detailed balance holds for moves
between~$S_1$ and~$S_2$, as long as we make use of the acceptance probability as
formulated in~\eqref{eqn:alpha rj}. We only work out the case in
which~$\alpha_m(x,x') = f_2(x',x)/f_1(x,x') \le 1$, the other one being entirely
analogous.

Recall that we are trying to prove that~\eqref{eqn:balance rj} holds, which in
the current context means showing that~\eqref{eqn:measure 1}
and~\eqref{eqn:measure 2} are equal when paired, respectively,
with~$\alpha_m(x,x')$ and~$\alpha(x',x)$:
\begin{multline*}
  \int_A \lambda_{n_1}(dx)\,p_1(x)j_1(x)
      \int_B \mu_1(dx')\,g_1(r_x^{-1}(x'))\alpha_m(x,x') \\
  \begin{aligned}
  &= \int_A \int_B \mu(dx,dx')\,p_1(x)j_1(x)g_1(r_x^{-1}(x'))\alpha_m(x,x') \\
  &= \int_A \int_B \mu(dx,dx')\,\frac{d\mu^*}{d\mu}
      p_2(x')j_2(x')g_2(r_{x'}^{-1}(x)). \\
  &= \int_B \int_A \mu^*(dx',dx)\,p_2(x')j_2(x')g_2(r_{x'}^{-1}(x)) \\
  &= \int_B \lambda_{n_2}(dx')\,p_2(x')j_2(x')
      \int_A \mu_2(dx)\,g_2(r_{x'}^{-1}(x))\alpha_m(x',x).
  \end{aligned}
\end{multline*}

\section{Some measure theory} %=================================================
\label{sec:measure}

These definitions and results all come from the
\emph{Random}\footnote{\url{http://www.randomservices.org/random}} website.

\subsection %===================================================================
  {\href{http://www.randomservices.org/random/foundations/Measure.html}
  {Algebras and measurable functions}}

Let~$S$ be a set, and~$\mc{S}$ a non-empty collection of subsets of~$S$.
\begin{defn}
$\mc{S}$ is an \emph{algebra} if it is closed under complements and unions.
\end{defn}
In particular, an algebra of sets is closed under a \emph{finite} number of
unions, intersections, or complements (or a combination thereof).

\begin{defn}
$\mc{S}$ is a \emph{$\sigma$-algebra} if it is closed under complements and
countable unions.
\end{defn}
It follows that a $\sigma$-algebra of sets is closed under countable
intersections too. If~$S$ is a set and~$\mc{S}$ a $\sigma$-algebra of subsets
of~$S$, then the pair~$(S,\mc{S})$ is called a \emph{measurable space}.

\begin{lem}
The intersection of a (possibly uncountable) collection of $\sigma$-algebras of
subsets of~$\mc{S}$ is itself a $\sigma$-algebra of subsets of~$\mc{S}$.
\end{lem}

Let~$\mc{B}$ be a collection of subsets of~$S$.
\begin{defn}
The $\sigma$-algebra \emph{generated by~$\mc{B}$} is the intersection of all
$\sigma$-algebras of subsets of~$S$ that contain~$\mc{B}$.
\end{defn}
The $\sigma$-algebra generated by the collection of open subsets of~$S$ is
called the \emph{Borel} $\sigma$-algebra of~$S$.

Suppose that~$(S,\mc{S})$ and~$(T,\mc{T})$ are measurable spaces.
\begin{defn}
A function~$f\colon S \to T$ is \emph{measurable} if $f^{-1}(B) \in \mc{S}$ for
every~$B \in \mc{T}$.
\end{defn}

\subsection %===================================================================
  {\href{http://www.randomservices.org/random/prob/Measure.html}
  {Measures}}

As above, let~$(S,\mc{S})$ and~$(T,\mc{T})$ be measurable spaces, and~$f\colon S
\to T$ be a measurable function.
\begin{defn}
A function~$\mu\colon \mc{S} \to [0,\infty]$ is a \emph{positive measure}
on~$(S,\mc{S})$ if $\mu(\emptyset) = 0$ and countable additivity holds (i.e.,
the measure of a countable union of disjoint subsets is the sum of the measures
of the subsets).
\end{defn}
A positive measure on~$(S,\mc{S})$ that also satisfies $\Pb(S) = 1$ is called a
\emph{probability measure}. The triple~$(S,\mc{S},\mu)$ is called a
\emph{measure space}.

\begin{lem}
If~$\mu$ is a positive measure on~$(S,\mc{S})$ and~$c > 0$, then~$c\mu$ is also
a positive measure on~$(S,\mc{S})$.
\end{lem}

\begin{lem}
If~$(R,\mc{R})$ is a measurable subspace of~$(S,\mc{S})$---i.e., $(R,\mc{R})$ is
a measurable space and~$\mc{R} \subseteq \mc{S}$---then~$\mu$ restricted
to~$\mc{R}$ is a positive measure on~$(R,\mc{R})$.
\end{lem}

\begin{thm}\label{thm:measures change}
The function~$\nu$ defined by $\nu(B) = \mu\lr*{f^{-1}(B)}$ for~$B \in \mc{T}$
is a positive measure on~$(T,\mc{T})$.
\end{thm}
The construction of a measure according to Theorem~\ref{thm:measures change} is
sometimes referred to as a \emph{change of variables}.

\subsection %===================================================================
  {\href{http://www.randomservices.org/random/dist/Integral.html}
  {Integrals}}

Let~$(S,\mc{S},\mu)$ be a measure space. Then the integral of a measurable
function~$f\colon S \to \mb{R}$ is denoted by~$\int_S f\,d\mu$, or~$\int_S
f(x)\,d\mu(x)$, or even~$\int_S \mu(dx)f(x)$, and we say that $f$ is
\emph{integrable} if the integral exists as a number in the set~$\mb{R} \cup
\{-\infty,\infty\}$.
\begin{defn}
If~$A \in \mc{S}$ then~$\int_S \symbf{1}_A\,d\mu = \mu(A)$.
\end{defn}

The integral with respect to a measure is defined in three stages: for simple
functions, non-negative functions, and then general functions.
\begin{defn}
A \emph{simple function} is a measurable, real-valued function with finite
range.
\end{defn}

\begin{lem}
A simple function~$f$ can be represented as a sum~$\sum_{i\in I}
a_i\symbf{1}_{A_i}$, where~$I$ is a finite index set, $a_i \in \mb{R}$ for
each~$i \in I$, and $\{\, A_i : i \in I \,\}$ is a collection of sets
in~$\,\mc{S}$ that partition~$S$.
\end{lem}
If the~$a_i$ are distinct and the~$A_i$ non-empty, the representation is called
\emph{canonical}.

Let~$f$ be a non-negative simple function with representation~$\sum_i
a_i\symbf{1}_{A_i}$, where~$a_i \ge 0$ for each~$i \in I$.
\begin{defn}
The integral of~$f$ with respect to~$\mu$ is
\[ \int_S f\,d\mu = \sum_{i\in I} a_i\mu(A_i). \]
\end{defn}
The above definition is consistent, in that the value of the integral is the
same for all representations of~$f$.

Next, suppose that $f\colon S \to [0,\infty)$ is measurable.
\begin{lem}
There exists an increasing sequence~$(f_1,f_2,\dots)$ of non-negative simple
functions such that~$f_n \to f$ on~$S$ as~$n \to \infty$.
\end{lem}
That is, every non-negative measurable function is the limit of a sequence of
non-negative simple functions.

\begin{defn}
The integral of a non-negative function~$f$ is
\[ \int_S f\,d\mu = \sup\left\{\, \int_S g\,d\mu
  : g \text{ is simple and } 0 \le g \le f \,\right\}. \]
\end{defn}

The above lemma and definition are linked by the \emph{monotone convergence
theorem}.
\begin{thm}
If~$(f_1,f_2,\dots)$ is an increasing sequence of non-negative simple functions
such that~$f_n \to f$ as~$n \to \infty$, then
\[ \int_S \lim_{n\to\infty} f_n\,d\mu = \lim_{n\to\infty} \int_S f_n\,d\mu. \]
\end{thm}

Finally, let~$f\colon S \to \mb{R}$ be any measurable function. For~$x \in
\mb{R}$, write the positive and negative parts of~$x$ as~$x^+ = \max(x,0)$
and~$x^- = \max(-x,0)$, so that $x = x^+ - x^-$ and $\abs{x} = x^+ + x^-$. This
definition extends to functions: $f^+$ is~$0$ wherever~$f \le 0$, and~$f^-$
is~$0$ wherever~$f \ge 0$.
\begin{defn}
The integral of a measurable function~$f$ is
\[ \int_S f\,\mu = \int_S f^+\,d\mu - \int_S f^-\,d\mu. \]
\end{defn}
The function~$f$ is integrable if and only if~$\int_S \abs{f}\,d\mu < \infty$.

\begin{defn}
The integral of~$f$ over a measurable subset~$A \in \mc{S}$ is
\[ \int_A f\,d\mu = \int_S \symbf{1}_A f\,d\mu. \]
\end{defn}
The integral over the union of finitely many disjoint, measurable subsets is the
sum of the integrals over the subsets.

(From \href{http://www.randomservices.org/random/dist/Properties.html}
{Section~2.11}.) Given a measure space~$(S,\mc{S},\mu)$, a measurable
space~$(T,\mc{T})$, and a measurable function~$f\colon S \to T$,
Theorem~\ref{thm:measures change} states that the function~$\nu(B) =
\mu(f^{-1}(B))$ is a positive measure on~$(T,\mc{T})$. This change of variables
theorem extends to integrals as follows.
\begin{thm}
If~$g\colon T \to \mb{R}$ is a measurable function, then, assuming the
integrals exist,
\[ \int_T g\,d\nu = \int_S (g \circ f)\,d\mu. \]
\end{thm}

\subsection %===================================================================
  {\href{http://www.randomservices.org/random/dist/Density.html}
  {Density functions}}

Let~$(S,\mc{S},\mu)$ be a measure space.
\begin{defn}
A subset~$A \in \mc{S}$ is a \emph{null} set of~$\mu$ if~$\mu(B) = 0$ for
every~$B \in \mc{S}$ such that $B \subseteq A$.
\end{defn}
\begin{defn}
Let~$\nu$ be another measure on~$(S,\mc{S})$. Then~$\nu$ is \emph{absolutely
continuous} with respect to~$\mu$---or \emph{dominated} by~$\mu$---if every null
set of~$\mu$ is also a null set of~$\nu$. On the other hand, $\mu$ and~$\nu$ are
\emph{mutually singular} if there exists an~$A \in \mc{S}$ such that~$A$ is null
for~$\mu$ and~$A^c$ is null for~$\nu$.
\end{defn}
Two measures~$\mu$ and~$\nu$ on~$(S,\mc{S})$ are said to be \emph{equivalent} if
they are absolutely continuous with respect to one another.

Let~$f\colon S \to \mb{R}$ be a measurable function whose integral with respect
to~$\mu$ exists.
\begin{defn}
The function~$\nu$ defined by
\[ \nu(A) = \int_A f\,d\mu \]
is a measure on~$(S,\mc{S})$ that is absolutely continuous with respect
to~$\nu$. The measurable function~$f$ is called a \emph{density function}
of~$\nu$ with respect to~$\mu$.
\end{defn}
If~$f$ is integrable, then $\nu$ is a finite measure; if~$f$ is non-negative,
$\nu$ is a positive measure; and if~$f$ is both non-negative and~$\int_S
f\,d\mu = 1$, then~$\nu$ is a probability measure. In the final case, $f$ is a
\emph{probability density function} of~$\nu$ relative to~$\mu$.

\begin{lem}
If~$f$ is a density function of~$\nu$ with respect to~$\mu$, then~$g\colon S \to
\mb{R}$ is a density function of~$\nu$ relative to~$\mu$ if and only if~$f = g$
almost everywhere on~$S$ with respect to~$\mu$.
\end{lem}

\begin{thm}
Let~$\nu$ be a measure on~$(S,\mc{S})$. Then~$\nu$ can be decomposed as~$\nu =
\nu_c + \nu_s$ such that~$\nu_c$ is absolutely continuous with respect to~$\mu$,
and~$\nu_s$ and~$\mu$ are mutually singular. Furthermore, $\nu_c$ has a density
function with respect to~$\mu$.
\end{thm}
The first and second parts of this theorem are called the \emph{Lebesgue
decomposition} and \emph{Radon-Nikodym} theorems respectively. In particular,
$\nu$ has a density function with respect to~$\mu$ if and only if~$\mu$
dominates~$\nu$. The derivative in this case is also referred to as the
\emph{Radon-Nikodym derivative} of~$\nu$ with respect to~$\mu$, and is sometimes
written as~$d\nu/d\mu$.

Suppose that~$\nu$ is absolutely continuous and has density function~$f$
relative to~$\mu$.
\begin{thm}
If~$g$ is a measurable function whose integral with respect to~$\nu$ exists,
then
\[ \int_S g\,d\nu = \int_S gf\,d\mu. \]
\end{thm}
This is the \emph{density theorem} for integrals, and is the reason for the
notation~$f = d\nu/d\mu$.

\subsection{The reals} %========================================================

(From \href{http://www.randomservices.org/random/foundations/Other.html}
{Section~0.12}.) Consider the set of open intervals on the real line:
\[ \mc{B} = \{\, (-\infty,a] : a \in \mb{R} \,\}
  \cup \{\, (a,b] : a,b \in \mb{R}, a < b \,\}
  \cup \{\, (b,\infty) : b \in \mb{R} \,\}. \]
\begin{lem}
The $\sigma$-algebra generated by~$\mc{B}$ is the $\sigma$-algebra of Borel sets
of~$\mb{R}$.
\end{lem}
In~$\mb{R}^n$, the collection that gives rise to the Borel $\sigma$-algebra
on~$\mb{R}^n$ is
\[ \mc{B}_n = \left\{\, \ts\prod_{i=1}^n A_i
  : A_i \in \mc{B} \text{ for each } i \in \{1,\dots,n\} \,\right\}. \]

(From \href{http://www.randomservices.org/random/prob/Probability.html}
{Section~1.3}.)
\begin{defn}
The standard $n$-dimensional measure on~$\mb{R}^n$ is called \emph{Lebesgue
measure}, defined by
\[ \lambda_n(A) = \int_A 1\,dx. \]
\end{defn}
In particular, $\lambda_1, \lambda_2$, and $\lambda_3$ correspond to length,
area, and volume, respectively.

\end{document}
